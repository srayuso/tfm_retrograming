{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import json\n",
    "import random\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as fn\n",
    "import pyspark.ml.feature as ft\n",
    "import pyspark.ml.clustering as clus\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"stats_and_sankey\").getOrCreate()\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "num_topics = 45\n",
    "file_in = \"atari_topic_names/\"\n",
    "models_in = \"atari_models/\"\n",
    "files_out = \"atari_sankey_and_stats/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cargando modelos en memoria...\n",
      "- vectorizer...\n",
      "- lda...\n"
     ]
    }
   ],
   "source": [
    "print('cargando modelos en memoria...')\n",
    "print('- vectorizer...')\n",
    "count_vectorizer_model_load = ft.CountVectorizerModel.load(models_in+'/count_vectorizer_model')\n",
    "print('- lda...')\n",
    "lda_model_load = clus.DistributedLDAModel.load(models_in+'/lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculando resumen de topics...\n"
     ]
    }
   ],
   "source": [
    "print(\"calculando resumen de topics...\")\n",
    "topics = lda_model_load.describeTopics(15).take(100)\n",
    "topic_terms = dict()\n",
    "topic_index = dict()\n",
    "for topic in topics:\n",
    "\tterms_in_topic = []\n",
    "\tfor i, v in enumerate(topic['termIndices']):\n",
    "\t\tif v not in topic_index:\n",
    "\t\t\ttopic_index[v] = (count_vectorizer_model_load.vocabulary[v], topic['termWeights'][i])\n",
    "\t\tterms_in_topic.append(topic_index[v])\n",
    "\n",
    "\ttopic_terms[topic['topic']] = terms_in_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(files_out+\"topic_terms_table.txt\",\"w+\")\n",
    "for i, terms in topic_terms.items():\n",
    "    termlist = [t[0] for t in terms[:7]]\n",
    "    f.write(\"#{}: {}\\n\".format(i, termlist))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(files_out+\"topic_terms.json\",\"w+\")\n",
    "f.write(json.dumps(topic_terms))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_weights = spark.sparkContext.parallelize(topic_terms.items()).toDF([\"topic_id\", \"tuple\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(file_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_weight = df.groupBy('topic_id')\\\n",
    "    .count()\\\n",
    "    .join(lookup_weights, [\"topic_id\"], \"left\")\\\n",
    "    .na.fill(\"tuple\", \"--\")\\\n",
    "    .withColumn('tuple', fn.explode(col('tuple')))\\\n",
    "    .select(col(\"topic_id\"), col(\"tuple._1\"), col('tuple._2'), col('count'))\\\n",
    "    .withColumn('weight_per_topic', col('_2') * col('count'))\\\n",
    "    .select(col('_1').alias('term'), col('weight_per_topic'))\\\n",
    "    .groupBy('term')\\\n",
    "    .agg(fn.sum('weight_per_topic').alias('weight'))\\\n",
    "    .take(45*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intento de equilibrar los tama침os un poco\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    return (1/(1 + (math.e)**(-x))) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_font_size = 100\n",
    "min_font_size = 10\n",
    "\n",
    "vega_term_weights = [{\"text\": r['term'], 'value': r['weight']} for r in df_term_weight]\n",
    "all_weights = [t['value'] for t in vega_term_weights]\n",
    "factor = (max(all_weights) - min(all_weights)) / (max_font_size - min_font_size)\n",
    "#term_weights = [{\"text\": r['term'], 'value': int(max_font_size * (sigmoid(r['weight'] / max(all_weights))) + min_font_size)} for r in df_term_weight]\n",
    "vega_term_weights = [{\"text\": r['term'], 'value': int(r['weight'] / factor + min_font_size)} for r in df_term_weight]\n",
    "\n",
    "# lo m치s c칩mo es renderizarlos en http://trifacta.github.io/vega/editor/\n",
    "f = open(files_out+\"vega_term_weights.json\",\"w+\")\n",
    "f.write(json.dumps(vega_term_weights))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_titles = {\n",
    "    0: \"Dev\",\n",
    "    1: \"HW\", # mods, repairs\n",
    "    2: \"HW\", # mods\n",
    "    3: \"Market\",\n",
    "    4: \"Games\",  # releases\n",
    "    5: \"Console Systems\", # console parts, details, photos, marketplace...\n",
    "    6: \"Music\",  # dev, soundtracks...\n",
    "    7: \"Games\",\n",
    "    8: \"Games\",\n",
    "    9: \"Dev\",\n",
    "    10: \"HW\",  # mods, upgrades\n",
    "    11: \"Games\", # discussion, expo, conventions\n",
    "    12: \"Dev\", # + Game discussion\n",
    "    13: \"Games\", # play, guides, cheats\n",
    "    14: \"Console Systems\", # o Games, es muy especifico\n",
    "    15: \"HW\", # especifico de joysticks\n",
    "    16: \"Social\", # games, console systems, discussion...\n",
    "    17: \"Social\", # discussion, what if, favorites...\n",
    "    18: \"Dev\",\n",
    "    19: \"HW\",\n",
    "    20: \"Social\", # console systems, mods, devs, help...\n",
    "    21: \"Console Systems\",\n",
    "    22: \"Social\", # expos, scans, magazines\n",
    "    23: \"HW\", \n",
    "    24: \"Social\", # games, hw\n",
    "    25: \"Social\", # games, recommendations\n",
    "    26: \"HW\", # mods, controllers\n",
    "    27: \"Market\",\n",
    "    28: \"HW\", # diy, problems..\n",
    "    29: \"Social\", # pero sobre temas de marketplace\n",
    "    30: \"Social\", # questions, offtopic, recommendations\n",
    "    31: \"Dev\", # DIY \n",
    "    32: \"Dev\", # con mucho HW\n",
    "    33: \"Console Systems\", # Console systems... very specific \n",
    "    34: \"Market\",\n",
    "    35: \"Social\", # collections sobre todo, dicussions\n",
    "    36: \"Social\", # nintendo, videos, \n",
    "    37: \"Games\", # discussion\n",
    "    38: \"Social\", # discussion\n",
    "    39: \"Social\", # collections, looking for stuff...\n",
    "    40: \"Games\", # expo, releases\n",
    "    41: \"Market\",\n",
    "    42: \"HW\", # sobre todo video, + marketplace\n",
    "    43: \"Social\", # discussion about everything\n",
    "    44: \"Console Systems\"\n",
    " }\n",
    "\n",
    "colors = ['rgb(141,211,199)','rgb(255,255,179)','rgb(190,186,218)','rgb(251,128,114)','rgb(128,177,211)','rgb(253,180,98)','rgb(179,222,105)','rgb(252,205,229)','rgb(217,217,217)','rgb(188,128,189)','rgb(204,235,197)']\n",
    "\n",
    "topic_list = {t for t in topic_titles.values()}\n",
    "topic_list.add('Non-English')\n",
    "topic_list.add('HSC')\n",
    "\n",
    "topic_color = {}\n",
    "for i, v in enumerate(topic_list):\n",
    "    topic_color[v] = colors[i]\n",
    "    \n",
    "lookup = spark.sparkContext.parallelize(topic_titles.items()).toDF([\"topic_id\", \"topic_title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links done ##############################################################\n",
      "nodes done ##############################################################\n",
      "order done ##############################################################\n",
      "all done ##############################################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "links_A = df\\\n",
    "\t.groupBy(col('forum_title'), col('topic_title'))\\\n",
    "\t.count()\\\n",
    "\t.sort(col('forum_title'), col('topic_title'))\\\n",
    "\t.take(1000)\n",
    "\n",
    "links = [{'source': e['forum_title'], 'target': e['topic_title'], 'value': e['count'], 'type': e['topic_title'], 'color': topic_color.get(e['topic_title'], 'rgb(0,0,0)')} for e in links_A]\n",
    "\n",
    "f = open(files_out+\"links.json\",\"w+\")\n",
    "f.write(json.dumps(links))\n",
    "f.close()\n",
    "\n",
    "\n",
    "print(\"links done ##############################################################\")\n",
    "\n",
    "node_set = set()\n",
    "for n in links:\n",
    "    node_set.add(n['source'])\n",
    "    node_set.add(n['target'])\n",
    "\n",
    "nodes = [{'id': t, 'title': t} for t in node_set]\n",
    "\n",
    "f = open(files_out+\"nodes.json\",\"w+\")\n",
    "f.write(json.dumps(nodes))\n",
    "f.close()\n",
    "\n",
    "\n",
    "print(\"nodes done ##############################################################\")\n",
    "\n",
    "df_forum_titles = df.select(col('forum_title')).distinct().sort(col('forum_title')).take(1000)\n",
    "order_forum_titles = [r['forum_title'] for r in df_forum_titles if r['forum_title'] in node_set]\n",
    "\n",
    "df_topic_titles = df.select(col('topic_title')).distinct().sort(col('topic_title')).take(1000)\n",
    "order_topic_titles = [r['topic_title'] for r in df_topic_titles if r['topic_title'] in node_set]\n",
    "\n",
    "\n",
    "order = [\n",
    "\t[\n",
    "\t\torder_forum_titles\n",
    "\t],\n",
    "\t[\n",
    "\t\torder_topic_titles\n",
    "\t]\n",
    "]\n",
    "\n",
    "f = open(files_out+\"order.json\",\"w+\")\n",
    "f.write(json.dumps(order))\n",
    "f.close()\n",
    "\n",
    "print(\"order done ##############################################################\")\n",
    "\n",
    "\n",
    "# topic_groups = {t for t in topic_titles.values()}\n",
    "# groups = [{'title': t, 'type': 'process', 'id': t, 'bundle': None, 'def_pos': None, 'nodes': [\"{}^*\".format(t)]} for t in topic_groups]\n",
    "\n",
    "# f = open(files_out+\"groups.json\",\"w+\")\n",
    "# f.write(json.dumps(groups))\n",
    "# f.close()\n",
    "\n",
    "\n",
    "# print(\"groups done ##############################################################\")\n",
    "\n",
    "\n",
    "sankey = {\n",
    "\t\"links\": links,\n",
    "\t\"order\": order,\n",
    "\t\"nodes\": nodes,\n",
    " #   \"groups\": groups\n",
    "}\n",
    "\n",
    "\n",
    "f = open(files_out+\"sankey.json\",\"w+\")\n",
    "f.write(json.dumps(sankey))\n",
    "f.close()\n",
    "\n",
    "print(\"all done ##############################################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|    topic_title| count|\n",
      "+---------------+------+\n",
      "|Console Systems|364803|\n",
      "|            Dev|339077|\n",
      "|          Games|566014|\n",
      "|            HSC|122358|\n",
      "|             HW|475348|\n",
      "|         Market|606303|\n",
      "|          Music| 28121|\n",
      "|    Non-English|  4332|\n",
      "|         Social|755891|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('topic_title').count().orderBy('topic_title').show(num_topics+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_freq = df.groupBy('topic_id').count()\n",
    "f = open(files_out+\"topic_freq.txt\",\"w+\")\n",
    "f.write('by topic_id:\\n')\n",
    "\n",
    "topic_freq_1 = topic_freq.orderBy('topic_id').take(100)\n",
    "for r in topic_freq_1:\n",
    "    f.write(\"#{}: {}\\n\".format(r['topic_id'], r['count']))\n",
    "\n",
    "\n",
    "f.write(('*' * 40) + '\\n')\n",
    "\n",
    "topic_freq_2 = topic_freq.orderBy('count').take(100)\n",
    "f.write('by count:\\n')\n",
    "for r in topic_freq_2:\n",
    "    f.write(\"#{}: {}\\n\".format(r['topic_id'], r['count']))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabando ejemplo de topics a disco...\n"
     ]
    }
   ],
   "source": [
    "threads = df\\\n",
    "    .groupBy(col('thread_code_url'), col('topic_id'), col('topic_title'))\\\n",
    "    .agg(fn.collect_list('post_text').alias('thread_text'), fn.min('post_date').alias('thread_date'), fn.min('thread_title').alias('thread_title'))\\\n",
    "    .withColumn('thread_text', fn.concat_ws(' ', col('thread_text')))\n",
    "threads.createOrReplaceTempView('threads')\n",
    "\n",
    "print(\"grabando ejemplo de topics a disco...\")\n",
    "summary = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  topic_id,\n",
    "  topic_title,\n",
    "  thread_title,\n",
    "  thread_text\n",
    "FROM (\n",
    "  SELECT\n",
    "    topic_id,\n",
    "    topic_title,\n",
    "    thread_text,\n",
    "    thread_title,\n",
    "    rank() OVER (PARTITION BY topic_id ORDER BY thread_date DESC) as rank\n",
    "  FROM threads) tmp\n",
    "WHERE\n",
    "  rank <= 20\n",
    "ORDER BY topic_title, topic_id ASC\n",
    "\"\"\").take(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = [\"<tr><td>t{}</td><td>{}</td><td>{}</td><td>{}</td></tr>\".format(r['topic_id'], r['topic_title'], r['thread_title'], r['thread_text']) for r in summary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"<html>\n",
    "<head></head>\n",
    "<body>\n",
    "<table border=1>{}</table>\n",
    "</body>\n",
    "</html>\"\"\".format(\"\\n\".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(files_out+\"summary.html\",\"w+\")\n",
    "f.write(html)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S칩lo si hace falta planchar otra vez los topic titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(file_in).drop('topic_title_2').drop('topic_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renombrando algunos topic titles...\n"
     ]
    }
   ],
   "source": [
    "df = df.join(lookup, [\"topic_id\"], \"left\").na.fill(\"topic_title\", \"--\")\n",
    "df.createOrReplaceTempView('df')\n",
    "print(\"renombrando algunos topic titles...\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW non_english AS\n",
    "SELECT *, IF(forum_code = '9-international/', 'Non-English', topic_title) AS topic_title_2 FROM df\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW hsc AS\n",
    "SELECT *,  IF(subforum_title like '%High Score Club%', 'HSC', topic_title_2) AS topic_title_3 FROM non_english\"\"\")\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM hsc\")\\\n",
    "        .drop('topic_title').drop('topic_title_2')\\\n",
    "        .withColumnRenamed('topic_title_3', 'topic_title')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
